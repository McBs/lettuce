{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import lettuce as lt\n",
    "from lettuce import D3Q19, Lattice, UnitConversion\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.623928022Z",
     "start_time": "2025-06-24T21:18:00.774578759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "class WallQuantitiesInternal:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lattice,\n",
    "        flow,\n",
    "        molecular_viscosity,\n",
    "        y_lattice=1.0,\n",
    "        wall='bottom',\n",
    "        kappa=0.41,\n",
    "        B=5.2,\n",
    "        max_iter=100,\n",
    "        tol=1e-8,\n",
    "        use_smagorinsky=False,\n",
    "        smagorinsky_collision_instance=None\n",
    "    ):\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.nu = molecular_viscosity\n",
    "        self.y = y_lattice\n",
    "        self.wall = wall\n",
    "        self.kappa = kappa\n",
    "        self.B = B\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.normal_axis = 1  # y-Achse\n",
    "\n",
    "        self.use_smagorinsky = use_smagorinsky\n",
    "        if self.use_smagorinsky:\n",
    "            if smagorinsky_collision_instance is None:\n",
    "                raise ValueError(\"Smagorinsky collision instance required if use_smagorinsky=True.\")\n",
    "            self.smagorinsky_collision = smagorinsky_collision_instance\n",
    "\n",
    "    def spalding_law(self, y_plus_grid_dist, u_mag_wall_parallel, nu_effective):\n",
    "        y_plus_grid_dist = torch.tensor(y_plus_grid_dist, device=u_mag_wall_parallel.device, dtype=u_mag_wall_parallel.dtype)\n",
    "        u_plus = (y_plus_grid_dist * u_mag_wall_parallel / nu_effective).clamp(min=1e-4).detach().clone().requires_grad_(False)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            ku = self.kappa * u_plus\n",
    "            exp_ku = torch.exp(ku)\n",
    "            spalding_term = u_plus + torch.exp(torch.tensor(-self.kappa * self.B, device=u_plus.device, dtype=u_plus.dtype)) * (\n",
    "                exp_ku - 1 - ku - 0.5 * ku**2 - (1/6) * ku**3\n",
    "            )\n",
    "            rhs = (y_plus_grid_dist * u_mag_wall_parallel) / (nu_effective * u_plus.clamp(min=1e-8))\n",
    "            f_eq_solve = spalding_term - rhs\n",
    "\n",
    "            d_spalding_term = 1 + torch.exp(torch.tensor(-self.kappa * self.B, device=u_plus.device, dtype=u_plus.dtype)) * (\n",
    "                self.kappa * exp_ku - self.kappa - self.kappa**2 * u_plus - 0.5 * self.kappa**3 * u_plus**2\n",
    "            )\n",
    "            d_rhs = - (y_plus_grid_dist * u_mag_wall_parallel) / (nu_effective * u_plus.clamp(min=1e-8)**2)\n",
    "            df_eq_solve = d_spalding_term - d_rhs\n",
    "\n",
    "            delta = f_eq_solve / torch.where(torch.abs(df_eq_solve) < 1e-10, torch.tensor(1e-10, device=f_eq_solve.device), df_eq_solve)\n",
    "            u_plus = (u_plus - delta).clamp(min=1e-4)\n",
    "\n",
    "            if torch.max(torch.abs(delta)) < self.tol:\n",
    "                break\n",
    "        return u_plus\n",
    "\n",
    "    def __call__(self, f_full_grid):\n",
    "        rho_full = self.lattice.rho(f_full_grid)\n",
    "        u_full = self.lattice.u(f_full_grid)\n",
    "\n",
    "        if rho_full.ndim == self.lattice.D + 1 and rho_full.shape[0] == 1:\n",
    "            rho_full = rho_full.squeeze(0)\n",
    "        if u_full.ndim == self.lattice.D + 1 and u_full.shape[1] == 1:\n",
    "            u_full = u_full.squeeze(1)\n",
    "\n",
    "        grid_spatial_dims = list(range(self.lattice.D))\n",
    "        spatial_idx_slice = [slice(None)] * self.lattice.D\n",
    "        spatial_idx_slice[self.normal_axis] = 1 if self.wall == \"bottom\" else -2\n",
    "\n",
    "        rho_f = rho_full[tuple(spatial_idx_slice)].flatten()\n",
    "        u_x_f = u_full[0][tuple(spatial_idx_slice)].flatten()\n",
    "        u_y_f = u_full[1][tuple(spatial_idx_slice)].flatten()\n",
    "        u_z_f = u_full[2][tuple(spatial_idx_slice)].flatten() if self.lattice.D == 3 else torch.zeros_like(u_x_f)\n",
    "\n",
    "        u_mag_wall_parallel = torch.sqrt(u_x_f**2 + u_z_f**2).clamp(min=1e-10)\n",
    "\n",
    "        if self.use_smagorinsky:\n",
    "            tau_eff_scalar = self.smagorinsky_collision.tau_eff\n",
    "            shape = u_full[0].shape\n",
    "            tau_eff_full_grid = torch.full(shape, tau_eff_scalar, device=u_full.device, dtype=u_full.dtype)\n",
    "            nu_eff_full_grid = (tau_eff_full_grid - 0.5) / 3.0\n",
    "            nu_eff_wall_layer = nu_eff_full_grid[tuple(spatial_idx_slice)].flatten()\n",
    "        else:\n",
    "            nu_eff_wall_layer = torch.full_like(u_mag_wall_parallel, self.nu)\n",
    "\n",
    "        u_plus = self.spalding_law(self.y, u_mag_wall_parallel, nu_eff_wall_layer)\n",
    "        u_tau = (u_mag_wall_parallel / u_plus).clamp(min=1e-8)\n",
    "        tau_w = rho_f * u_tau**2\n",
    "\n",
    "        safe_u_mag = torch.where(u_mag_wall_parallel < 1e-10, torch.tensor(1.0, device=u_mag_wall_parallel.device, dtype=u_mag_wall_parallel.dtype), u_mag_wall_parallel)\n",
    "        tau_x = - (u_x_f / safe_u_mag) * tau_w\n",
    "        tau_z = - (u_z_f / safe_u_mag) * tau_w\n",
    "\n",
    "        tau_x_wall = torch.zeros_like(u_full[0])\n",
    "        tau_z_wall = torch.zeros_like(u_full[2] if self.lattice.D == 3 else u_full[0])\n",
    "\n",
    "        if self.lattice.D == 3:\n",
    "            target_shape_slice = tau_x_wall[tuple(spatial_idx_slice)].shape\n",
    "            tau_x_wall[tuple(spatial_idx_slice)] = tau_x.reshape(target_shape_slice)\n",
    "            tau_z_wall[tuple(spatial_idx_slice)] = tau_z.reshape(target_shape_slice)\n",
    "        else:\n",
    "            raise ValueError(\"Only 3D supported for this current implementation of WallQuantitiesInternal.\")\n",
    "\n",
    "        return {\n",
    "            \"tau_x\": tau_x_wall,\n",
    "            \"tau_z\": tau_z_wall,\n",
    "            \"u_tau\": u_tau\n",
    "        }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.633393754Z",
     "start_time": "2025-06-24T21:18:01.632367237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class WallFunctionBoundaryTest:\n",
    "    def __init__(self, mask, lattice, flow, wall='bottom',\n",
    "                 kappa=0.41, B=5.2, max_iter=100, tol=1e-8):\n",
    "        self.mask = lattice.convert_to_tensor(mask)\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.wall = wall\n",
    "        self.kappa = kappa\n",
    "        self.B = B\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "        self.tau_x = None\n",
    "        self.tau_z = None\n",
    "\n",
    "        self.u_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.y_plus_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.Re_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "        self.previous_u_tau_mean = torch.tensor(0.0, device=lattice.device, dtype=lattice.dtype)\n",
    "\n",
    "    def solve_u_tau_exact(self, y, u, nu):\n",
    "        device = u.device\n",
    "        dtype = u.dtype\n",
    "        kappa = self.kappa\n",
    "        B = self.B\n",
    "        A = torch.exp(torch.tensor(-kappa * B, device=device, dtype=dtype))\n",
    "\n",
    "        u_tau = u.clone().clamp(min=1e-4)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            u_plus = u / u_tau\n",
    "            ku = kappa * u_plus\n",
    "            exp_ku = torch.exp(ku)\n",
    "\n",
    "            f_rhs = u_plus + A * (exp_ku - 1 - ku - 0.5 * ku ** 2 - (1 / 6) * ku ** 3)\n",
    "            lhs = y * u_tau / nu\n",
    "            residual = lhs - f_rhs\n",
    "\n",
    "            d_f_rhs_duplus = 1 + A * (kappa * exp_ku - kappa - kappa**2 * u_plus - 0.5 * kappa**3 * u_plus**2)\n",
    "            d_uplus_du_tau = -u / u_tau**2\n",
    "            df_du_tau = d_f_rhs_duplus * d_uplus_du_tau\n",
    "\n",
    "            d_lhs_du_tau = y / nu\n",
    "            total_derivative = d_lhs_du_tau - df_du_tau\n",
    "\n",
    "            safe_deriv = torch.where(torch.abs(total_derivative) < 1e-12,\n",
    "                                     torch.tensor(1e-12, device=device, dtype=dtype),\n",
    "                                     total_derivative)\n",
    "\n",
    "            delta = residual / safe_deriv\n",
    "            u_tau_new = (u_tau - delta).clamp(min=1e-5)\n",
    "\n",
    "            if torch.max(torch.abs(delta)) < self.tol:\n",
    "                break\n",
    "\n",
    "            u_tau = u_tau_new\n",
    "\n",
    "        return u_tau\n",
    "\n",
    "    def __call__(self, f):\n",
    "        if self.wall == 'bottom':\n",
    "            f17_old = f[17, self.mask].clone()\n",
    "            f16_old = f[16, self.mask].clone()\n",
    "            f10_old = f[10, self.mask].clone()\n",
    "            f8_old  = f[8, self.mask].clone()\n",
    "\n",
    "        elif self.wall == 'top':\n",
    "            f15_old = f[15, self.mask].clone()\n",
    "            f18_old = f[18, self.mask].clone()\n",
    "            f7_old  = f[7, self.mask].clone()\n",
    "            f9_old  = f[9, self.mask].clone()\n",
    "        else:\n",
    "            raise ValueError(\"wall must be 'bottom' or 'top'\")\n",
    "\n",
    "\n",
    "\n",
    "        rho = self.lattice.rho(f)\n",
    "        rho = rho[:,self.mask]\n",
    "        u = self.lattice.u(f)\n",
    "\n",
    "        u_x = u[0][self.mask]\n",
    "        u_z = u[2][self.mask]\n",
    "        u_mag_parallel = torch.sqrt(u_x**2 + u_z**2).clamp(min=1e-10)\n",
    "\n",
    "        y = torch.tensor(1.0, device=f.device, dtype=f.dtype)\n",
    "        nu = torch.tensor(self.flow.units.viscosity_lu, device=f.device, dtype=f.dtype)\n",
    "\n",
    "        if nu <= 0 or torch.isnan(nu) or torch.isinf(nu):\n",
    "            self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            return f\n",
    "\n",
    "        u_tau = self.solve_u_tau_exact(y, u_mag_parallel, nu)\n",
    "        tau_w = rho * u_tau**2\n",
    "\n",
    "        if torch.isnan(tau_w).any() or torch.isinf(tau_w).any():\n",
    "            self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            return f\n",
    "\n",
    "        safe_u = torch.where(u_mag_parallel < 1e-10, torch.tensor(1.0, device=f.device), u_mag_parallel)\n",
    "        safe_u = u_mag_parallel\n",
    "        tau_x = - (u_x / safe_u) * tau_w\n",
    "        tau_z = - (u_z / safe_u) * tau_w\n",
    "\n",
    "        tau_x_field = torch.zeros_like(u[0])\n",
    "        tau_z_field = torch.zeros_like(u[2] if self.lattice.D == 3 else u[0])\n",
    "        tau_x_field[self.mask] = 0.5*tau_x\n",
    "        tau_z_field[self.mask] = 0.5*tau_z\n",
    "\n",
    "        f = torch.where(self.mask, f[self.lattice.stencil.opposite], f)\n",
    "\n",
    "        if self.wall == 'bottom':\n",
    "            f[15, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[8,  self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[7,  self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[17, self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[9,  self.mask] = f8_old - tau_x_field[self.mask]\n",
    "            f[10, self.mask] = f8_old - tau_x_field[self.mask]\n",
    "        elif self.wall == 'top':\n",
    "            f[17, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[9,  self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[10, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[15, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[8,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "            f[7,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "\n",
    "        self.tau_x = tau_x_field\n",
    "        self.tau_z = tau_z_field\n",
    "        self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "        self.u_tau_mean = u_tau.mean()\n",
    "        # Lokales y_plus wie gehabt\n",
    "        self.y_plus_mean = (y * u_tau / nu).mean()\n",
    "\n",
    "        # Korrektes Re_tau\n",
    "        H = self.flow.resolution_y / 2  # fÃ¼r y-Achse\n",
    "        self.Re_tau_mean = (u_tau * H / nu).mean()\n",
    "\n",
    "        print(\"Re tau:\" + str(self.Re_tau_mean))\n",
    "        if torch.isnan(self.u_tau_mean) or torch.isinf(self.u_tau_mean) or \\\n",
    "           torch.isnan(self.y_plus_mean) or torch.isinf(self.y_plus_mean) or \\\n",
    "           torch.isnan(self.Re_tau_mean) or torch.isinf(self.Re_tau_mean):\n",
    "            self.u_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.y_plus_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "            self.Re_tau_mean = torch.tensor(0.0, device=f.device, dtype=f.dtype)\n",
    "\n",
    "\n",
    "        return f\n",
    "\n",
    "    def make_no_collision_mask(self, f_shape):\n",
    "        assert self.mask.shape == f_shape[1:]\n",
    "        return self.mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.688974001Z",
     "start_time": "2025-06-24T21:18:01.686983001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class WallFunctionBoundaryTest2:\n",
    "    def __init__(self, mask, lattice, flow, wall='bottom', apply_wfb_correction=True,\n",
    "                 smagorinsky_collision_instance=None):\n",
    "        self.mask = lattice.convert_to_tensor(mask)\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.wall = wall\n",
    "        self.apply_wfb_correction = apply_wfb_correction\n",
    "        self.normal_axis = 1\n",
    "\n",
    "        self.tau_x = None\n",
    "        self.tau_z = None\n",
    "        self.u_tau_mean = None\n",
    "        self.y_plus_mean = None\n",
    "        self.Re_tau_mean = None\n",
    "\n",
    "        self.use_smagorinsky = smagorinsky_collision_instance is not None\n",
    "\n",
    "        # Konstruktion der WallQuantitiesInternal â€“ immer\n",
    "        self.wall_quantities_internal = WallQuantitiesInternal(\n",
    "            lattice=self.lattice,\n",
    "            flow=self.flow,\n",
    "            molecular_viscosity=self.flow.units.viscosity_lu,\n",
    "            wall=self.wall,\n",
    "            y_lattice=1.0,\n",
    "            use_smagorinsky=self.use_smagorinsky,\n",
    "            smagorinsky_collision_instance=smagorinsky_collision_instance\n",
    "        )\n",
    "\n",
    "    def set_smagorinsky_collision(self, collision):\n",
    "        self.smagorinsky_collision = collision\n",
    "        self.wall_quantities_internal = WallQuantitiesInternal(\n",
    "            lattice=self.lattice,\n",
    "            flow=self.flow,\n",
    "            molecular_viscosity=self.flow.units.viscosity_lu,\n",
    "            wall=self.wall,\n",
    "            y_lattice=1.0,\n",
    "            smagorinsky_collision_instance=collision\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, f):\n",
    "        # --- 1. Klonen der Originalverteilungen fÃ¼r spÃ¤tere Korrektur ---\n",
    "\n",
    "        if self.wall == 'bottom':\n",
    "            f17_old = f [17, self.mask].clone()\n",
    "            f16_old = f[16, self.mask].clone()\n",
    "            f10_old = f[10, self.mask].clone()\n",
    "            f8_old  = f[8, self.mask].clone()\n",
    "        elif self.wall == 'top':\n",
    "            f15_old = f[15, self.mask].clone()\n",
    "            f18_old = f[18, self.mask].clone()\n",
    "            f7_old  = f[7, self.mask].clone()\n",
    "            f9_old  = f[9, self.mask].clone()\n",
    "        else:\n",
    "            raise ValueError(\"wall must be 'bottom' or 'top'\")\n",
    "\n",
    "        # --- 2. Wall Quantities intern berechnen ---\n",
    "        results = self.wall_quantities_internal(f)\n",
    "        tau_x_field = 0.5 * results[\"tau_x\"]\n",
    "        tau_z_field = 0.5 * results[\"tau_z\"]\n",
    "\n",
    "        # --- 3. Free-Slip Bounce-Back anwenden ---\n",
    "        f = torch.where(self.mask, f[self.lattice.stencil.opposite], f)\n",
    "\n",
    "        # --- 4. Additive Wandkorrektur ---\n",
    "        if self.wall == 'bottom':\n",
    "            f[15, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f17_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[8,  self.mask] = f16_old - tau_z_field[self.mask]\n",
    "            f[7,  self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[17, self.mask] = f10_old + tau_x_field[self.mask]\n",
    "            f[9,  self.mask] = f8_old - tau_x_field[self.mask]\n",
    "            f[10, self.mask] = f8_old - tau_x_field[self.mask]\n",
    "        elif self.wall == 'top':\n",
    "            f[17, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[18, self.mask] = f15_old + tau_z_field[self.mask]\n",
    "            f[16, self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[9,  self.mask] = f18_old - tau_z_field[self.mask]\n",
    "            f[10, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[15, self.mask] = f7_old + tau_x_field[self.mask]\n",
    "            f[8,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "            f[7,  self.mask] = f9_old - tau_x_field[self.mask]\n",
    "\n",
    "        # --- 5. Ergebnisse fÃ¼r Reporter speichern ---\n",
    "        self.tau_x = tau_x_field\n",
    "        self.tau_z = tau_z_field\n",
    "\n",
    "\n",
    "        self.u_tau_mean = results[\"u_tau\"].mean()\n",
    "        self.previous_u_tau_mean = self.u_tau_mean.clone().detach()\n",
    "\n",
    "        self.y_plus_mean = (self.wall_quantities_internal.y * results[\"u_tau\"] / self.wall_quantities_internal.nu).mean()\n",
    "        self.Re_tau_mean = (results[\"u_tau\"] * self.wall_quantities_internal.y / self.wall_quantities_internal.nu).mean()\n",
    "\n",
    "        return f\n",
    "\n",
    "    def make_no_collision_mask(self, f_shape):\n",
    "        \"\"\"\n",
    "        Diese Boundary-Methode liefert die Maske der Wandknoten,\n",
    "        auf denen der Kollisionsschritt der Hauptsimulation Ã¼bersprungen werden soll.\n",
    "        Diese Klasse operiert auf diesen Wandknoten selbst.\n",
    "        \"\"\"\n",
    "        assert self.mask.shape == f_shape[1:]\n",
    "        # KORREKTUR: Muss die Maske der eigenen Wandknoten (self.mask) zurÃ¼ckgeben.\n",
    "        return self.mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.697869304Z",
     "start_time": "2025-06-24T21:18:01.696149218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ChannelFlow3DTest:\n",
    "    def __init__(self, resolution_x, resolution_y, resolution_z,\n",
    "                 reynolds_number, mach_number, lattice, char_length_lu,\n",
    "                 boundary=None,  # jetzt erlaubt: externe Ãœbergabe\n",
    "                 boundaries=None  # neu: direkt Ã¼bergebene Instanzen\n",
    "                 ):\n",
    "        self.resolution_x = resolution_x\n",
    "        self.resolution_y = resolution_y\n",
    "        self.resolution_z = resolution_z\n",
    "        self._boundary = boundary  # z.â€¯B. \"wallfunction\"\n",
    "        self._external_boundaries = boundaries  # die Instanzen direkt\n",
    "        self._boundaries = None  # wird ggf. erzeugt\n",
    "\n",
    "        self.units = UnitConversion(\n",
    "            lattice,\n",
    "            reynolds_number=reynolds_number,\n",
    "            mach_number=mach_number,\n",
    "            characteristic_length_lu=char_length_lu,\n",
    "            characteristic_length_pu=1,\n",
    "            characteristic_velocity_pu=1\n",
    "        )\n",
    "\n",
    "        self._mask = np.zeros(shape=(self.resolution_x, self.resolution_y, self.resolution_z), dtype=bool)\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "    @mask.setter\n",
    "    def mask(self, m):\n",
    "        assert isinstance(m, np.ndarray) and m.shape == (self.resolution_x, self.resolution_y, self.resolution_z)\n",
    "        self._mask = m.astype(bool)\n",
    "\n",
    "    def initial_solution(self, grid):\n",
    "        xg, yg, zg = grid\n",
    "        p = np.ones_like(xg)[None, ...]\n",
    "        nx, ny, nz = self.resolution_x, self.resolution_y, self.resolution_z\n",
    "\n",
    "        u = np.zeros((3, nx, ny, nz))\n",
    "\n",
    "        # --- ðŸ“ Poiseuille-Profil (in x-Richtung) ---\n",
    "        y_normalized = yg / yg.max()\n",
    "        u_base = y_normalized * (1 - y_normalized)\n",
    "        u[0] = u_base * (1 - self.mask.astype(float))  # u_x = BasisstrÃ¶mung\n",
    "\n",
    "        # --- ðŸŽ›ï¸ Sinusmoden-StÃ¶rung (3D) ---\n",
    "        A_sin = 0.5  # 5% Amplitude\n",
    "        Lx, Ly, Lz = xg.max(), yg.max(), zg.max()\n",
    "        sinus_modes = [(1, 1, 1), (2, 2, 3), (3, 2, 1)]\n",
    "\n",
    "        for kx, ky, kz in sinus_modes:\n",
    "            phase = 2 * np.pi * np.random.rand()\n",
    "            mode = np.sin(2 * np.pi * (kx * xg / Lx + ky * yg / Ly + kz * zg / Lz) + phase)\n",
    "            envelope = y_normalized * (1 - y_normalized)\n",
    "            u[0] += A_sin * mode * envelope  # nur u_x gestÃ¶rt, kannst du erweitern\n",
    "\n",
    "        # --- ðŸŒªï¸ Vektorpotential Ïˆ (3 Komponenten fÃ¼r Curl in 3D) ---\n",
    "        A_psi = 1\n",
    "        random_psi = ((np.random.rand(3, nx, ny, nz) - 0.5) * 2)\n",
    "\n",
    "        # Wandgewichtung in y und z\n",
    "        y_weight = np.exp(-((y_normalized - 0.0) / 0.2) ** 2) + np.exp(-((y_normalized - 1.0) / 0.2) ** 2)\n",
    "        y_weight /= y_weight.max()\n",
    "\n",
    "        z_normalized = zg / zg.max()\n",
    "        z_weight = np.exp(-((z_normalized - 0.5) / 0.3) ** 2)\n",
    "        z_weight /= z_weight.max()\n",
    "\n",
    "        weight = y_weight * z_weight\n",
    "        random_psi *= weight[None, :, :, :]\n",
    "\n",
    "        # FFT-Filterung (3D)\n",
    "        k0 = np.sqrt(nx ** 2 + ny ** 2 + nz ** 2)\n",
    "        psi_filtered = np.empty_like(random_psi)\n",
    "        for d in range(3):\n",
    "            psi_hat = np.fft.fftn(random_psi[d])\n",
    "            kx = np.fft.fftfreq(nx).reshape(-1, 1, 1)\n",
    "            ky = np.fft.fftfreq(ny).reshape(1, -1, 1)\n",
    "            kz = np.fft.fftfreq(nz).reshape(1, 1, -1)\n",
    "            kabs = np.sqrt((kx * nx) ** 2 + (ky * ny) ** 2 + (kz * nz) ** 2)\n",
    "            filter_mask = np.exp(-kabs / (0.3 * k0))\n",
    "            psi_hat *= filter_mask\n",
    "            psi_hat[0, 0, 0] = 0\n",
    "            psi_filtered[d] = np.real(np.fft.ifftn(psi_hat))\n",
    "\n",
    "        # --- ðŸŒ€ Curl(Ïˆ): u = âˆ‡ Ã— Ïˆ ---\n",
    "        u_psi = np.zeros_like(u)\n",
    "        u_psi[0] = np.gradient(psi_filtered[2], axis=1) - np.gradient(psi_filtered[1], axis=2)  # u_x\n",
    "        u_psi[1] = np.gradient(psi_filtered[0], axis=2) - np.gradient(psi_filtered[2], axis=0)  # u_y\n",
    "        u_psi[2] = np.gradient(psi_filtered[1], axis=0) - np.gradient(psi_filtered[0], axis=1)  # u_z\n",
    "\n",
    "        # Normierung\n",
    "        umax_psi = np.max(np.sqrt(np.sum(u_psi ** 2, axis=0)))\n",
    "        if umax_psi > 0:\n",
    "            u_psi *= A_psi / umax_psi\n",
    "\n",
    "        # --- Ãœberlagerung: Basis + Sine + Curl ---\n",
    "        u += u_psi\n",
    "        # 2D: Nullsetzen der Wandgeschwindigkeit\n",
    "\n",
    "        u[:, :, 0, :] = 0.0  # untere Wand (y=0)\n",
    "        u[:, :, -1, :] = 0.0  # obere Wand (y=Ny-1)\n",
    "\n",
    "        return p, u\n",
    "\n",
    "    @property\n",
    "    def grid(self):\n",
    "        stop_x = self.resolution_x / self.units.characteristic_length_lu\n",
    "        stop_y = self.resolution_y / self.units.characteristic_length_lu\n",
    "        stop_z = self.resolution_z / self.units.characteristic_length_lu\n",
    "\n",
    "        x = np.linspace(0, stop_x, num=self.resolution_x, endpoint=False)\n",
    "        y = np.linspace(0, stop_y, num=self.resolution_y, endpoint=False)\n",
    "        z = np.linspace(0, stop_z, num=self.resolution_z, endpoint=False)\n",
    "\n",
    "        return np.meshgrid(x, y, z, indexing='ij')\n",
    "\n",
    "    @property\n",
    "    def boundaries(self):\n",
    "\n",
    "        return self._external_boundaries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.707758716Z",
     "start_time": "2025-06-24T21:18:01.706025789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class WallQuantitiesTest:\n",
    "    def __init__(self, lattice, flow, boundary):\n",
    "        self.lattice = lattice\n",
    "        self.flow = flow\n",
    "        self.boundary = boundary\n",
    "\n",
    "    def __call__(self, f):\n",
    "        try:\n",
    "            # Hier die hasattr-PrÃ¼fung ENTFERNEN. Die Attribute existieren jetzt immer, da sie in __init__\n",
    "            # von WallFunctionBoundaryTest mit 0.0 initialisiert werden.\n",
    "\n",
    "            # Stattdessen prÃ¼fen, ob die Werte noch die initialen Nullen sind oder NaN/Inf\n",
    "            # (assuming WallFunctionBoundaryTest initializes to torch.tensor(0.0))\n",
    "            if (self.boundary.u_tau_mean.item() == 0.0 and self.boundary.y_plus_mean.item() == 0.0 and self.boundary.Re_tau_mean.item() == 0.0) or \\\n",
    "               torch.isnan(self.boundary.u_tau_mean) or torch.isinf(self.boundary.u_tau_mean):\n",
    "                return torch.zeros(3, dtype=f.dtype, device=f.device)\n",
    "\n",
    "            u_tau_mean = self.boundary.u_tau_mean\n",
    "            y_plus_mean = self.boundary.y_plus_mean\n",
    "            re_tau_mean = self.boundary.Re_tau_mean\n",
    "\n",
    "\n",
    "            return torch.stack([\n",
    "                u_tau_mean,\n",
    "                y_plus_mean,\n",
    "                re_tau_mean,\n",
    "            ])\n",
    "\n",
    "        except Exception as e:\n",
    "            return torch.zeros(3, dtype=f.dtype, device=f.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.757291626Z",
     "start_time": "2025-06-24T21:18:01.710205291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICH FUNKTIONIERE MIT PULLEN\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--vtkdir\", type=str, default=\"./output/\")\n",
    "parser.add_argument(\"--csvdir\", type=str, default=\"./output/\")\n",
    "parser.add_argument(\"--nout\", type=int, default=100)\n",
    "parser.add_argument(\"--nvtk\", type=int, default=100)\n",
    "parser.add_argument(\"--tmax\", type=int, default=20)\n",
    "parser.add_argument(\"--Re\", type=int, default=13800)\n",
    "parser.add_argument(\"--collision_operator\", type=str, default=\"BGK\")\n",
    "parser.add_argument(\"--Precision\", type=str, default=\"Double\")\n",
    "parser.add_argument(\"--Mach\", type=float, default=0.1)\n",
    "parser.add_argument(\"--h\", type=int, default=20, help=\"Halbe KanalhÃ¶he in LU\")\n",
    "parser.add_argument(\"--bbtype\", type=str, default=\"wallfunction\", choices=[\"halfway\", \"fullway\", \"wallfunction\", \"freeslip\"],\n",
    "                    help=\"Typ der Bounce-Back-Randbedingung\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "args = vars(args)\n",
    "\n",
    "print(\"ICH FUNKTIONIERE MIT PULLEN\")\n",
    "\n",
    "\n",
    "\n",
    "# Einheiten und AuflÃ¶sung\n",
    "h = args[\"h\"]                      # KanalhalbhÃ¶he in LU\n",
    "res_y = 2 * h                     # y: volle KanalhÃ¶he\n",
    "res_x = int(2*np.pi * h)\n",
    "res_z = int(np.pi * h)\n",
    "\n",
    "# Restliche Parameter\n",
    "Re = args[\"Re\"]\n",
    "basedir = args[\"vtkdir\"]\n",
    "csvdir = args[\"csvdir\"]\n",
    "nout = args[\"nout\"]\n",
    "nvtk = args[\"nvtk\"]\n",
    "tmax = args[\"tmax\"]\n",
    "Precision = args[\"Precision\"]\n",
    "collision_operator = args[\"collision_operator\"]\n",
    "Mach = args[\"Mach\"]\n",
    "bbtype = args[\"bbtype\"]\n",
    "# PrÃ¤zision\n",
    "if Precision == \"Single\":\n",
    "    dtype = torch.float32\n",
    "elif Precision == \"Double\":\n",
    "    dtype = torch.float64\n",
    "elif Precision == \"Half\":\n",
    "    dtype = torch.float16\n",
    "\n",
    "\n",
    "Re_tau = 180\n",
    "\n",
    "smagorinsky_constant = 0.17\n",
    "\n",
    "delta_x = 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-24T21:18:01.757702869Z",
     "start_time": "2025-06-24T21:18:01.750675987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ist nicht verfÃ¼gbar. Verwende CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/lettuce/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps     time     GlobalMeanUXReporter\n",
      "steps     time     WallQuantities\n",
      "steps     time     WallQuantities\n",
      "steps     time     IncompressibleKineticEnergy\n",
      "Re tau:tensor(53.6610, dtype=torch.float64)\n",
      "Re tau:tensor(53.7423, dtype=torch.float64)\n",
      "Re tau:tensor(56.3691, dtype=torch.float64)\n",
      "Re tau:tensor(56.4223, dtype=torch.float64)\n",
      "Re tau:tensor(56.4230, dtype=torch.float64)\n",
      "Re tau:tensor(56.6129, dtype=torch.float64)\n",
      "Re tau:tensor(53.2607, dtype=torch.float64)\n",
      "Re tau:tensor(53.5100, dtype=torch.float64)\n",
      "Re tau:tensor(55.1698, dtype=torch.float64)\n",
      "Re tau:tensor(55.1129, dtype=torch.float64)\n",
      "Re tau:tensor(52.5760, dtype=torch.float64)\n",
      "Re tau:tensor(52.6286, dtype=torch.float64)\n",
      "Re tau:tensor(54.9782, dtype=torch.float64)\n",
      "Re tau:tensor(54.5352, dtype=torch.float64)\n",
      "Re tau:tensor(53.6690, dtype=torch.float64)\n",
      "Re tau:tensor(53.3834, dtype=torch.float64)\n",
      "Re tau:tensor(55.6692, dtype=torch.float64)\n",
      "Re tau:tensor(55.4140, dtype=torch.float64)\n",
      "Re tau:tensor(52.8934, dtype=torch.float64)\n",
      "Re tau:tensor(53.0861, dtype=torch.float64)\n",
      "Re tau:tensor(56.4056, dtype=torch.float64)\n",
      "Re tau:tensor(56.5100, dtype=torch.float64)\n",
      "Re tau:tensor(52.2080, dtype=torch.float64)\n",
      "Re tau:tensor(52.3204, dtype=torch.float64)\n",
      "Re tau:tensor(54.8472, dtype=torch.float64)\n",
      "Re tau:tensor(54.8987, dtype=torch.float64)\n",
      "Re tau:tensor(51.6838, dtype=torch.float64)\n",
      "Re tau:tensor(51.8884, dtype=torch.float64)\n",
      "Re tau:tensor(55.7602, dtype=torch.float64)\n",
      "Re tau:tensor(55.5163, dtype=torch.float64)\n",
      "Re tau:tensor(53.0143, dtype=torch.float64)\n",
      "Re tau:tensor(53.3807, dtype=torch.float64)\n",
      "Re tau:tensor(55.6377, dtype=torch.float64)\n",
      "Re tau:tensor(55.7146, dtype=torch.float64)\n",
      "Re tau:tensor(52.3446, dtype=torch.float64)\n",
      "Re tau:tensor(52.2735, dtype=torch.float64)\n",
      "Re tau:tensor(55.9290, dtype=torch.float64)\n",
      "Re tau:tensor(55.9368, dtype=torch.float64)\n",
      "Re tau:tensor(52.6840, dtype=torch.float64)\n",
      "Re tau:tensor(53.4024, dtype=torch.float64)\n",
      "Re tau:tensor(56.1250, dtype=torch.float64)\n",
      "Re tau:tensor(55.8891, dtype=torch.float64)\n",
      "Re tau:tensor(53.5579, dtype=torch.float64)\n",
      "Re tau:tensor(53.7011, dtype=torch.float64)\n",
      "Re tau:tensor(56.4619, dtype=torch.float64)\n",
      "Re tau:tensor(56.0014, dtype=torch.float64)\n",
      "Re tau:tensor(53.7094, dtype=torch.float64)\n",
      "Re tau:tensor(53.4273, dtype=torch.float64)\n",
      "Re tau:tensor(56.3002, dtype=torch.float64)\n",
      "Re tau:tensor(56.1507, dtype=torch.float64)\n",
      "Re tau:tensor(53.0601, dtype=torch.float64)\n",
      "Re tau:tensor(53.0992, dtype=torch.float64)\n",
      "Re tau:tensor(55.9313, dtype=torch.float64)\n",
      "Re tau:tensor(56.0254, dtype=torch.float64)\n",
      "Re tau:tensor(53.4382, dtype=torch.float64)\n",
      "Re tau:tensor(53.2174, dtype=torch.float64)\n",
      "Re tau:tensor(56.3378, dtype=torch.float64)\n",
      "Re tau:tensor(56.0472, dtype=torch.float64)\n",
      "Re tau:tensor(52.9790, dtype=torch.float64)\n",
      "Re tau:tensor(52.8607, dtype=torch.float64)\n",
      "Re tau:tensor(55.8485, dtype=torch.float64)\n",
      "Re tau:tensor(55.7397, dtype=torch.float64)\n",
      "Re tau:tensor(52.9501, dtype=torch.float64)\n",
      "Re tau:tensor(53.0289, dtype=torch.float64)\n",
      "Re tau:tensor(56.0455, dtype=torch.float64)\n",
      "Re tau:tensor(56.2546, dtype=torch.float64)\n",
      "Re tau:tensor(52.9645, dtype=torch.float64)\n",
      "Re tau:tensor(53.1323, dtype=torch.float64)\n",
      "Re tau:tensor(55.9533, dtype=torch.float64)\n",
      "Re tau:tensor(56.0566, dtype=torch.float64)\n",
      "Re tau:tensor(53.2792, dtype=torch.float64)\n",
      "Re tau:tensor(53.5232, dtype=torch.float64)\n",
      "Re tau:tensor(56.0669, dtype=torch.float64)\n",
      "Re tau:tensor(56.2347, dtype=torch.float64)\n",
      "Re tau:tensor(53.2246, dtype=torch.float64)\n",
      "Re tau:tensor(53.5012, dtype=torch.float64)\n",
      "Re tau:tensor(56.2697, dtype=torch.float64)\n",
      "Re tau:tensor(56.0020, dtype=torch.float64)\n",
      "Re tau:tensor(53.4088, dtype=torch.float64)\n",
      "Re tau:tensor(53.3318, dtype=torch.float64)\n",
      "Re tau:tensor(56.7405, dtype=torch.float64)\n",
      "Re tau:tensor(56.5288, dtype=torch.float64)\n",
      "Re tau:tensor(53.9420, dtype=torch.float64)\n",
      "Re tau:tensor(53.9803, dtype=torch.float64)\n",
      "Re tau:tensor(56.7499, dtype=torch.float64)\n",
      "Re tau:tensor(56.6954, dtype=torch.float64)\n",
      "Re tau:tensor(54.2004, dtype=torch.float64)\n",
      "Re tau:tensor(54.2430, dtype=torch.float64)\n",
      "Re tau:tensor(57.0897, dtype=torch.float64)\n",
      "Re tau:tensor(57.2426, dtype=torch.float64)\n",
      "Re tau:tensor(54.0983, dtype=torch.float64)\n",
      "Re tau:tensor(54.2949, dtype=torch.float64)\n",
      "Re tau:tensor(57.0044, dtype=torch.float64)\n",
      "Re tau:tensor(57.0146, dtype=torch.float64)\n",
      "Re tau:tensor(54.4204, dtype=torch.float64)\n",
      "Re tau:tensor(54.7123, dtype=torch.float64)\n",
      "Re tau:tensor(57.0977, dtype=torch.float64)\n",
      "Re tau:tensor(57.2412, dtype=torch.float64)\n",
      "Re tau:tensor(54.9286, dtype=torch.float64)\n",
      "Re tau:tensor(54.9837, dtype=torch.float64)\n",
      "Re tau:tensor(57.3065, dtype=torch.float64)\n",
      "Re tau:tensor(57.7019, dtype=torch.float64)\n",
      "Re tau:tensor(54.5305, dtype=torch.float64)\n",
      "Re tau:tensor(54.7117, dtype=torch.float64)\n",
      "Re tau:tensor(57.6173, dtype=torch.float64)\n",
      "Re tau:tensor(57.5933, dtype=torch.float64)\n",
      "Re tau:tensor(54.7787, dtype=torch.float64)\n",
      "Re tau:tensor(54.9750, dtype=torch.float64)\n",
      "Re tau:tensor(57.5317, dtype=torch.float64)\n",
      "Re tau:tensor(57.7132, dtype=torch.float64)\n",
      "Re tau:tensor(55.4372, dtype=torch.float64)\n",
      "Re tau:tensor(55.5823, dtype=torch.float64)\n",
      "Re tau:tensor(57.7696, dtype=torch.float64)\n",
      "Re tau:tensor(57.7929, dtype=torch.float64)\n",
      "Re tau:tensor(55.2045, dtype=torch.float64)\n",
      "Re tau:tensor(55.3949, dtype=torch.float64)\n",
      "Re tau:tensor(57.9423, dtype=torch.float64)\n",
      "Re tau:tensor(58.2186, dtype=torch.float64)\n",
      "Re tau:tensor(55.3411, dtype=torch.float64)\n",
      "Re tau:tensor(55.4808, dtype=torch.float64)\n",
      "Re tau:tensor(57.8504, dtype=torch.float64)\n",
      "Re tau:tensor(58.2666, dtype=torch.float64)\n",
      "Re tau:tensor(55.2782, dtype=torch.float64)\n",
      "Re tau:tensor(55.6179, dtype=torch.float64)\n",
      "Re tau:tensor(57.9655, dtype=torch.float64)\n",
      "Re tau:tensor(58.2798, dtype=torch.float64)\n",
      "Re tau:tensor(55.8445, dtype=torch.float64)\n",
      "Re tau:tensor(55.6959, dtype=torch.float64)\n",
      "Re tau:tensor(58.3733, dtype=torch.float64)\n",
      "Re tau:tensor(58.1590, dtype=torch.float64)\n",
      "Re tau:tensor(55.8160, dtype=torch.float64)\n",
      "Re tau:tensor(55.9903, dtype=torch.float64)\n",
      "Re tau:tensor(58.6134, dtype=torch.float64)\n",
      "Re tau:tensor(58.5704, dtype=torch.float64)\n",
      "Re tau:tensor(56.2755, dtype=torch.float64)\n",
      "Re tau:tensor(56.1670, dtype=torch.float64)\n",
      "Re tau:tensor(58.2880, dtype=torch.float64)\n",
      "Re tau:tensor(58.5044, dtype=torch.float64)\n",
      "Re tau:tensor(55.9995, dtype=torch.float64)\n",
      "Re tau:tensor(56.3136, dtype=torch.float64)\n",
      "Re tau:tensor(58.6300, dtype=torch.float64)\n",
      "Re tau:tensor(58.5792, dtype=torch.float64)\n",
      "Re tau:tensor(56.2490, dtype=torch.float64)\n",
      "Re tau:tensor(56.2359, dtype=torch.float64)\n",
      "Re tau:tensor(58.8013, dtype=torch.float64)\n",
      "Re tau:tensor(58.9199, dtype=torch.float64)\n",
      "Re tau:tensor(56.3639, dtype=torch.float64)\n",
      "Re tau:tensor(56.4521, dtype=torch.float64)\n",
      "Re tau:tensor(59.2241, dtype=torch.float64)\n",
      "Re tau:tensor(59.0595, dtype=torch.float64)\n",
      "Re tau:tensor(56.7671, dtype=torch.float64)\n",
      "Re tau:tensor(56.6659, dtype=torch.float64)\n",
      "Re tau:tensor(58.9252, dtype=torch.float64)\n",
      "Re tau:tensor(58.9895, dtype=torch.float64)\n",
      "Re tau:tensor(56.6555, dtype=torch.float64)\n",
      "Re tau:tensor(56.8212, dtype=torch.float64)\n",
      "Re tau:tensor(59.3461, dtype=torch.float64)\n",
      "Re tau:tensor(59.5578, dtype=torch.float64)\n",
      "Re tau:tensor(57.1878, dtype=torch.float64)\n",
      "Re tau:tensor(57.1939, dtype=torch.float64)\n",
      "Re tau:tensor(59.5325, dtype=torch.float64)\n",
      "Re tau:tensor(59.7222, dtype=torch.float64)\n",
      "Re tau:tensor(57.3412, dtype=torch.float64)\n",
      "Re tau:tensor(57.5403, dtype=torch.float64)\n",
      "Re tau:tensor(59.6856, dtype=torch.float64)\n",
      "Re tau:tensor(59.8630, dtype=torch.float64)\n",
      "Re tau:tensor(57.5676, dtype=torch.float64)\n",
      "Re tau:tensor(57.6827, dtype=torch.float64)\n",
      "Re tau:tensor(59.8788, dtype=torch.float64)\n",
      "Re tau:tensor(59.9486, dtype=torch.float64)\n",
      "Re tau:tensor(57.7209, dtype=torch.float64)\n",
      "Re tau:tensor(57.8175, dtype=torch.float64)\n",
      "Re tau:tensor(59.9563, dtype=torch.float64)\n",
      "Re tau:tensor(59.9536, dtype=torch.float64)\n",
      "Re tau:tensor(58.1572, dtype=torch.float64)\n",
      "Re tau:tensor(58.0912, dtype=torch.float64)\n",
      "Re tau:tensor(60.3518, dtype=torch.float64)\n",
      "Re tau:tensor(60.5211, dtype=torch.float64)\n",
      "Re tau:tensor(58.5528, dtype=torch.float64)\n",
      "Re tau:tensor(58.5050, dtype=torch.float64)\n",
      "Re tau:tensor(60.5919, dtype=torch.float64)\n",
      "Re tau:tensor(60.6415, dtype=torch.float64)\n",
      "Re tau:tensor(58.4677, dtype=torch.float64)\n",
      "Re tau:tensor(58.6150, dtype=torch.float64)\n",
      "Re tau:tensor(60.6774, dtype=torch.float64)\n",
      "Re tau:tensor(60.7832, dtype=torch.float64)\n",
      "Re tau:tensor(58.5938, dtype=torch.float64)\n",
      "Re tau:tensor(58.6484, dtype=torch.float64)\n",
      "Re tau:tensor(60.7802, dtype=torch.float64)\n",
      "Re tau:tensor(60.9314, dtype=torch.float64)\n",
      "Re tau:tensor(58.7215, dtype=torch.float64)\n",
      "Re tau:tensor(58.7522, dtype=torch.float64)\n",
      "Re tau:tensor(60.7764, dtype=torch.float64)\n",
      "Re tau:tensor(60.9775, dtype=torch.float64)\n",
      "Re tau:tensor(58.9435, dtype=torch.float64)\n",
      "Re tau:tensor(58.7871, dtype=torch.float64)\n",
      "Re tau:tensor(61.0914, dtype=torch.float64)\n",
      "Re tau:tensor(61.2071, dtype=torch.float64)\n",
      "Re tau:tensor(59.0777, dtype=torch.float64)\n",
      "Re tau:tensor(59.3454, dtype=torch.float64)\n",
      "Re tau:tensor(61.1429, dtype=torch.float64)\n",
      "Re tau:tensor(61.3362, dtype=torch.float64)\n",
      "Re tau:tensor(59.3675, dtype=torch.float64)\n",
      "Re tau:tensor(59.6629, dtype=torch.float64)\n",
      "Re tau:tensor(61.2747, dtype=torch.float64)\n",
      "Re tau:tensor(61.3699, dtype=torch.float64)\n",
      "Re tau:tensor(59.4945, dtype=torch.float64)\n",
      "Re tau:tensor(59.4665, dtype=torch.float64)\n",
      "Re tau:tensor(61.5614, dtype=torch.float64)\n",
      "Re tau:tensor(61.6611, dtype=torch.float64)\n",
      "Re tau:tensor(59.9105, dtype=torch.float64)\n",
      "Re tau:tensor(59.9027, dtype=torch.float64)\n",
      "Re tau:tensor(61.8248, dtype=torch.float64)\n",
      "Re tau:tensor(61.9281, dtype=torch.float64)\n",
      "Re tau:tensor(60.3117, dtype=torch.float64)\n",
      "Re tau:tensor(60.5061, dtype=torch.float64)\n",
      "Re tau:tensor(62.0756, dtype=torch.float64)\n",
      "Re tau:tensor(62.1364, dtype=torch.float64)\n",
      "Re tau:tensor(60.3503, dtype=torch.float64)\n",
      "Re tau:tensor(60.4305, dtype=torch.float64)\n",
      "Re tau:tensor(62.2277, dtype=torch.float64)\n",
      "Re tau:tensor(62.3141, dtype=torch.float64)\n",
      "Re tau:tensor(60.5411, dtype=torch.float64)\n",
      "Re tau:tensor(60.6917, dtype=torch.float64)\n",
      "Re tau:tensor(62.6926, dtype=torch.float64)\n",
      "Re tau:tensor(62.5324, dtype=torch.float64)\n",
      "Re tau:tensor(60.9533, dtype=torch.float64)\n",
      "Re tau:tensor(60.9281, dtype=torch.float64)\n",
      "Re tau:tensor(62.7250, dtype=torch.float64)\n",
      "Re tau:tensor(63.0880, dtype=torch.float64)\n",
      "Re tau:tensor(61.1663, dtype=torch.float64)\n",
      "Re tau:tensor(61.1727, dtype=torch.float64)\n",
      "Re tau:tensor(62.9580, dtype=torch.float64)\n",
      "Re tau:tensor(62.8892, dtype=torch.float64)\n",
      "Re tau:tensor(61.3679, dtype=torch.float64)\n",
      "Re tau:tensor(61.5340, dtype=torch.float64)\n",
      "Re tau:tensor(62.8188, dtype=torch.float64)\n",
      "Re tau:tensor(63.1644, dtype=torch.float64)\n",
      "Re tau:tensor(61.4602, dtype=torch.float64)\n",
      "Re tau:tensor(61.5352, dtype=torch.float64)\n",
      "Re tau:tensor(63.2489, dtype=torch.float64)\n",
      "Re tau:tensor(63.3772, dtype=torch.float64)\n",
      "Re tau:tensor(61.8906, dtype=torch.float64)\n",
      "Re tau:tensor(62.1352, dtype=torch.float64)\n",
      "Re tau:tensor(63.5248, dtype=torch.float64)\n",
      "Re tau:tensor(63.5902, dtype=torch.float64)\n",
      "Re tau:tensor(61.9506, dtype=torch.float64)\n",
      "Re tau:tensor(62.1224, dtype=torch.float64)\n",
      "Re tau:tensor(63.5887, dtype=torch.float64)\n",
      "Re tau:tensor(63.9381, dtype=torch.float64)\n",
      "Re tau:tensor(62.1124, dtype=torch.float64)\n",
      "Re tau:tensor(62.4626, dtype=torch.float64)\n",
      "Re tau:tensor(63.7079, dtype=torch.float64)\n",
      "Re tau:tensor(63.9122, dtype=torch.float64)\n",
      "Re tau:tensor(62.4070, dtype=torch.float64)\n",
      "Re tau:tensor(62.4004, dtype=torch.float64)\n",
      "Re tau:tensor(63.6564, dtype=torch.float64)\n",
      "Re tau:tensor(63.8525, dtype=torch.float64)\n",
      "Re tau:tensor(62.2612, dtype=torch.float64)\n",
      "Re tau:tensor(62.5131, dtype=torch.float64)\n",
      "Re tau:tensor(63.7179, dtype=torch.float64)\n",
      "Re tau:tensor(63.9479, dtype=torch.float64)\n",
      "Re tau:tensor(62.3408, dtype=torch.float64)\n",
      "Re tau:tensor(62.4568, dtype=torch.float64)\n",
      "Re tau:tensor(63.8635, dtype=torch.float64)\n",
      "Re tau:tensor(64.1276, dtype=torch.float64)\n",
      "Re tau:tensor(62.5147, dtype=torch.float64)\n",
      "Re tau:tensor(62.7885, dtype=torch.float64)\n",
      "Re tau:tensor(64.1510, dtype=torch.float64)\n",
      "Re tau:tensor(64.3560, dtype=torch.float64)\n",
      "Re tau:tensor(62.6906, dtype=torch.float64)\n",
      "Re tau:tensor(62.8678, dtype=torch.float64)\n",
      "Re tau:tensor(64.1855, dtype=torch.float64)\n",
      "Re tau:tensor(64.5037, dtype=torch.float64)\n",
      "Re tau:tensor(62.8486, dtype=torch.float64)\n",
      "Re tau:tensor(63.2156, dtype=torch.float64)\n",
      "Re tau:tensor(64.4738, dtype=torch.float64)\n",
      "Re tau:tensor(64.6211, dtype=torch.float64)\n",
      "Re tau:tensor(63.0479, dtype=torch.float64)\n",
      "Re tau:tensor(63.3456, dtype=torch.float64)\n",
      "Re tau:tensor(64.6350, dtype=torch.float64)\n",
      "Re tau:tensor(64.7523, dtype=torch.float64)\n",
      "Re tau:tensor(63.3215, dtype=torch.float64)\n",
      "Re tau:tensor(63.6006, dtype=torch.float64)\n",
      "Re tau:tensor(64.7165, dtype=torch.float64)\n",
      "Re tau:tensor(64.9020, dtype=torch.float64)\n",
      "Re tau:tensor(63.4953, dtype=torch.float64)\n",
      "Re tau:tensor(63.6048, dtype=torch.float64)\n",
      "Re tau:tensor(64.8429, dtype=torch.float64)\n",
      "Re tau:tensor(64.9447, dtype=torch.float64)\n",
      "Re tau:tensor(63.7763, dtype=torch.float64)\n",
      "Re tau:tensor(64.0330, dtype=torch.float64)\n",
      "Re tau:tensor(64.9563, dtype=torch.float64)\n",
      "Re tau:tensor(65.3520, dtype=torch.float64)\n",
      "Re tau:tensor(63.9381, dtype=torch.float64)\n",
      "Re tau:tensor(64.2358, dtype=torch.float64)\n",
      "Re tau:tensor(65.4117, dtype=torch.float64)\n",
      "Re tau:tensor(65.4741, dtype=torch.float64)\n",
      "Re tau:tensor(64.2114, dtype=torch.float64)\n",
      "Re tau:tensor(64.5184, dtype=torch.float64)\n",
      "Re tau:tensor(65.7304, dtype=torch.float64)\n",
      "Re tau:tensor(65.8042, dtype=torch.float64)\n",
      "Re tau:tensor(64.1709, dtype=torch.float64)\n",
      "Re tau:tensor(64.3870, dtype=torch.float64)\n",
      "Re tau:tensor(65.5765, dtype=torch.float64)\n",
      "Re tau:tensor(65.7111, dtype=torch.float64)\n",
      "Re tau:tensor(64.5875, dtype=torch.float64)\n",
      "Re tau:tensor(64.7689, dtype=torch.float64)\n",
      "Re tau:tensor(65.5947, dtype=torch.float64)\n",
      "Re tau:tensor(65.8619, dtype=torch.float64)\n",
      "Re tau:tensor(64.6404, dtype=torch.float64)\n",
      "Re tau:tensor(64.7389, dtype=torch.float64)\n",
      "Re tau:tensor(66.0121, dtype=torch.float64)\n",
      "Re tau:tensor(66.0752, dtype=torch.float64)\n",
      "Re tau:tensor(64.6623, dtype=torch.float64)\n",
      "Re tau:tensor(65.1276, dtype=torch.float64)\n",
      "Re tau:tensor(66.0992, dtype=torch.float64)\n",
      "Re tau:tensor(66.1642, dtype=torch.float64)\n",
      "Re tau:tensor(64.9062, dtype=torch.float64)\n",
      "Re tau:tensor(65.3136, dtype=torch.float64)\n",
      "Re tau:tensor(66.2440, dtype=torch.float64)\n",
      "Re tau:tensor(66.2841, dtype=torch.float64)\n",
      "Re tau:tensor(65.0890, dtype=torch.float64)\n",
      "Re tau:tensor(65.4854, dtype=torch.float64)\n",
      "Re tau:tensor(66.3789, dtype=torch.float64)\n",
      "Re tau:tensor(66.6046, dtype=torch.float64)\n",
      "Re tau:tensor(65.2607, dtype=torch.float64)\n",
      "Re tau:tensor(65.5695, dtype=torch.float64)\n",
      "Re tau:tensor(66.4422, dtype=torch.float64)\n",
      "Re tau:tensor(66.6737, dtype=torch.float64)\n",
      "Re tau:tensor(65.5145, dtype=torch.float64)\n",
      "Re tau:tensor(65.8760, dtype=torch.float64)\n",
      "Re tau:tensor(66.8052, dtype=torch.float64)\n",
      "Re tau:tensor(66.9209, dtype=torch.float64)\n",
      "Re tau:tensor(65.6325, dtype=torch.float64)\n",
      "Re tau:tensor(66.0248, dtype=torch.float64)\n",
      "Re tau:tensor(66.8272, dtype=torch.float64)\n",
      "Re tau:tensor(67.0395, dtype=torch.float64)\n",
      "Re tau:tensor(65.8109, dtype=torch.float64)\n",
      "Re tau:tensor(66.2256, dtype=torch.float64)\n",
      "Re tau:tensor(66.8992, dtype=torch.float64)\n",
      "Re tau:tensor(67.1756, dtype=torch.float64)\n",
      "Re tau:tensor(65.9833, dtype=torch.float64)\n",
      "Re tau:tensor(66.4762, dtype=torch.float64)\n",
      "Re tau:tensor(67.2872, dtype=torch.float64)\n",
      "Re tau:tensor(67.3259, dtype=torch.float64)\n",
      "Re tau:tensor(66.3258, dtype=torch.float64)\n",
      "Re tau:tensor(66.6091, dtype=torch.float64)\n",
      "Re tau:tensor(67.1953, dtype=torch.float64)\n",
      "Re tau:tensor(67.5984, dtype=torch.float64)\n",
      "Re tau:tensor(66.4125, dtype=torch.float64)\n",
      "Re tau:tensor(66.8764, dtype=torch.float64)\n",
      "Re tau:tensor(67.5274, dtype=torch.float64)\n",
      "Re tau:tensor(67.6619, dtype=torch.float64)\n",
      "Re tau:tensor(66.4969, dtype=torch.float64)\n",
      "Re tau:tensor(66.8725, dtype=torch.float64)\n",
      "Re tau:tensor(67.6036, dtype=torch.float64)\n",
      "Re tau:tensor(67.7502, dtype=torch.float64)\n",
      "Re tau:tensor(66.9318, dtype=torch.float64)\n",
      "Re tau:tensor(67.2050, dtype=torch.float64)\n",
      "Re tau:tensor(67.8012, dtype=torch.float64)\n",
      "Re tau:tensor(67.9780, dtype=torch.float64)\n",
      "Re tau:tensor(66.8779, dtype=torch.float64)\n",
      "Re tau:tensor(67.2710, dtype=torch.float64)\n",
      "Re tau:tensor(67.9818, dtype=torch.float64)\n",
      "Re tau:tensor(68.0022, dtype=torch.float64)\n",
      "Re tau:tensor(67.2765, dtype=torch.float64)\n",
      "Re tau:tensor(67.5229, dtype=torch.float64)\n",
      "Re tau:tensor(68.0555, dtype=torch.float64)\n",
      "Re tau:tensor(68.2209, dtype=torch.float64)\n",
      "Re tau:tensor(67.2988, dtype=torch.float64)\n",
      "Re tau:tensor(67.6542, dtype=torch.float64)\n",
      "Re tau:tensor(68.2838, dtype=torch.float64)\n",
      "Re tau:tensor(68.4117, dtype=torch.float64)\n",
      "Re tau:tensor(67.5967, dtype=torch.float64)\n",
      "Re tau:tensor(67.8618, dtype=torch.float64)\n",
      "Re tau:tensor(68.5131, dtype=torch.float64)\n",
      "Re tau:tensor(68.5568, dtype=torch.float64)\n",
      "Re tau:tensor(67.7052, dtype=torch.float64)\n",
      "Re tau:tensor(67.9488, dtype=torch.float64)\n",
      "Re tau:tensor(68.6908, dtype=torch.float64)\n",
      "Re tau:tensor(68.9621, dtype=torch.float64)\n",
      "Re tau:tensor(67.9328, dtype=torch.float64)\n",
      "Re tau:tensor(68.2851, dtype=torch.float64)\n",
      "Re tau:tensor(68.8591, dtype=torch.float64)\n",
      "Re tau:tensor(69.0944, dtype=torch.float64)\n",
      "Re tau:tensor(68.0472, dtype=torch.float64)\n",
      "Re tau:tensor(68.3976, dtype=torch.float64)\n",
      "Re tau:tensor(69.1224, dtype=torch.float64)\n",
      "Re tau:tensor(69.1891, dtype=torch.float64)\n",
      "Re tau:tensor(68.3591, dtype=torch.float64)\n",
      "Re tau:tensor(68.5274, dtype=torch.float64)\n",
      "Re tau:tensor(68.9992, dtype=torch.float64)\n",
      "Re tau:tensor(69.3409, dtype=torch.float64)\n",
      "Re tau:tensor(68.5017, dtype=torch.float64)\n",
      "Re tau:tensor(68.7817, dtype=torch.float64)\n",
      "Re tau:tensor(69.2527, dtype=torch.float64)\n",
      "Re tau:tensor(69.4889, dtype=torch.float64)\n",
      "Re tau:tensor(68.8457, dtype=torch.float64)\n",
      "Re tau:tensor(68.9579, dtype=torch.float64)\n",
      "Re tau:tensor(69.4426, dtype=torch.float64)\n",
      "Re tau:tensor(69.6236, dtype=torch.float64)\n",
      "Re tau:tensor(68.7875, dtype=torch.float64)\n",
      "Re tau:tensor(69.1579, dtype=torch.float64)\n",
      "Re tau:tensor(69.6968, dtype=torch.float64)\n",
      "Re tau:tensor(69.8199, dtype=torch.float64)\n",
      "Re tau:tensor(69.0854, dtype=torch.float64)\n",
      "Re tau:tensor(69.2334, dtype=torch.float64)\n",
      "Re tau:tensor(69.7740, dtype=torch.float64)\n",
      "Re tau:tensor(69.8971, dtype=torch.float64)\n",
      "Re tau:tensor(69.2994, dtype=torch.float64)\n",
      "Re tau:tensor(69.4824, dtype=torch.float64)\n",
      "Re tau:tensor(69.9195, dtype=torch.float64)\n",
      "Re tau:tensor(70.1029, dtype=torch.float64)\n",
      "Re tau:tensor(69.4619, dtype=torch.float64)\n",
      "Re tau:tensor(69.6211, dtype=torch.float64)\n",
      "Re tau:tensor(69.9862, dtype=torch.float64)\n",
      "Re tau:tensor(70.2804, dtype=torch.float64)\n",
      "Re tau:tensor(69.3920, dtype=torch.float64)\n",
      "Re tau:tensor(69.7391, dtype=torch.float64)\n",
      "Re tau:tensor(70.0514, dtype=torch.float64)\n",
      "Re tau:tensor(70.3491, dtype=torch.float64)\n",
      "Re tau:tensor(69.7112, dtype=torch.float64)\n",
      "Re tau:tensor(69.9819, dtype=torch.float64)\n",
      "Re tau:tensor(70.1586, dtype=torch.float64)\n",
      "Re tau:tensor(70.4328, dtype=torch.float64)\n",
      "Re tau:tensor(69.6531, dtype=torch.float64)\n",
      "Re tau:tensor(70.0684, dtype=torch.float64)\n",
      "Re tau:tensor(70.4855, dtype=torch.float64)\n",
      "Re tau:tensor(70.5962, dtype=torch.float64)\n",
      "Re tau:tensor(69.9710, dtype=torch.float64)\n",
      "Re tau:tensor(70.1262, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# CUDA & PrÃ¤zision\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA ist verfÃ¼gbar. Verwende GPU.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA ist nicht verfÃ¼gbar. Verwende CPU.\")\n",
    "\n",
    "dtype = torch.float64  # FÃ¼r StabilitÃ¤t bei hohen Re\n",
    "lattice = Lattice(D3Q19(), device=device, dtype=dtype)\n",
    "\n",
    "# ðŸ§± DomÃ¤nenmaÃŸe & Setup\n",
    "h = args[\"h\"]\n",
    "res_y = 2 * h\n",
    "res_x = int(2 * np.pi * h)\n",
    "res_z = int(np.pi * h)\n",
    "\n",
    "# ðŸ‘‡ Maske fÃ¼r Wandfunktion vorbereiten\n",
    "grid_x, grid_y, grid_z = np.meshgrid(\n",
    "    np.arange(res_x), np.arange(res_y), np.arange(res_z), indexing='ij'\n",
    ")\n",
    "\n",
    "mask_bottom = np.zeros_like(grid_x, dtype=bool)\n",
    "mask_bottom[:, 0, :] = True  # Erste Fluidzelle oben\n",
    "\n",
    "mask_top = np.zeros_like(grid_x, dtype=bool)\n",
    "mask_top[:, -1, :] = True  # Erste Fluidzelle unten\n",
    "\n",
    "# ðŸ”§ Boundaries manuell erzeugen\n",
    "wfb_bottom = WallFunctionBoundaryTest(mask=mask_bottom, lattice=lattice, flow=None, wall='bottom')\n",
    "wfb_top    = WallFunctionBoundaryTest(mask=mask_top,    lattice=lattice, flow=None, wall='top')\n",
    "\n",
    "# ðŸŒŠ Flow erzeugen, Boundaries Ã¼bergeben\n",
    "flow = ChannelFlow3DTest(\n",
    "    resolution_x=res_x,\n",
    "    resolution_y=res_y,\n",
    "    resolution_z=res_z,\n",
    "    reynolds_number=Re,\n",
    "    mach_number=Mach,\n",
    "    lattice=lattice,\n",
    "    char_length_lu=res_y,\n",
    "    boundaries=[wfb_bottom, wfb_top]\n",
    ")\n",
    "\n",
    "# ðŸ§© Boundaries kennen jetzt den Flow\n",
    "wfb_bottom.flow = flow\n",
    "wfb_top.flow = flow\n",
    "\n",
    "# ðŸ§  Check: IDs vergleichen\n",
    "\n",
    "# ðŸ“ˆ Reporter: Global Mean Ux\n",
    "global_mean_ux_reporter = lt.GlobalMeanUXReporter(lattice, flow)\n",
    "\n",
    "# ðŸ“Š Wall Quantities Reporter (lesen von denselben Objekten)\n",
    "wq_bottom = lt.WallQuantities(lattice=lattice, flow=flow, boundary=wfb_bottom)\n",
    "wq_top = lt.WallQuantities(lattice=lattice, flow=flow, boundary=wfb_top)\n",
    "\n",
    "\n",
    "\n",
    "# ðŸŒ€ Adaptive Force mit denselben Boundaries\n",
    "adaptive_force_instance = lt.AdaptiveForce(\n",
    "    lattice=lattice,\n",
    "    flow=flow,\n",
    "    target_u_m_lu=flow.units.convert_velocity_to_lu(1.0),\n",
    "    wall_bottom=wfb_bottom,\n",
    "    wall_top=wfb_top,\n",
    "    global_ux_reporter=global_mean_ux_reporter,\n",
    "    base_lbm_tau_lu=flow.units.relaxation_parameter_lu\n",
    ")\n",
    "\n",
    "# âš™ï¸ Kollision & Simulation\n",
    "collision = lt.BGKCollision(lattice, tau=flow.units.relaxation_parameter_lu, force=adaptive_force_instance)\n",
    "\n",
    "\n",
    "streaming = lt.StandardStreaming(lattice)\n",
    "\n",
    "\n",
    "\n",
    "simulation = lt.Simulation(flow=flow, lattice=lattice, collision=collision, streaming=streaming)\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ“¤ Reporter einfÃ¼gen\n",
    "simulation.reporters.append(lt.ObservableReporter(global_mean_ux_reporter, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(wq_bottom, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(wq_top, interval=1, out=None))\n",
    "simulation.reporters.append(lt.ObservableReporter(lt.observables.IncompressibleKineticEnergy(lattice, flow), interval=100, out=None))\n",
    "\n",
    "# ðŸ“¦ VTK\n",
    "steps = int(flow.units.convert_time_to_lu(tmax))\n",
    "vtk_reporter = lt.VTKReporter(lattice=lattice, flow=flow, interval=max(1, int(steps/100)), filename_base=basedir + \"/output\")\n",
    "simulation.reporters.append(vtk_reporter)\n",
    "\n",
    "# â–¶ï¸ Simulation starten\n",
    "simulation.initialize_f_neq()\n",
    "mlups = simulation.step(num_steps=steps)\n",
    "\n",
    "# ðŸ§¾ Ergebnisse abspeichern\n",
    "wq_top_arr = np.array(simulation.reporters[2].out)\n",
    "wq_bottom_arr = np.array(simulation.reporters[1].out)\n",
    "ux_mean_arr = np.array(simulation.reporters[0].out)\n",
    "\n",
    "with open(csvdir + 'uxmean.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(ux_mean_arr)\n",
    "with open(csvdir + 'WallQuantitiesTop.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(wq_top_arr)\n",
    "with open(csvdir + 'WallQuantitiesBottom.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(wq_bottom_arr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-24T21:18:01.750864984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(wq_bottom)\n",
    "# Beispiel: Daten laden\n",
    "data = (wq_bottom_arr+wq_top_arr)/2\n",
    "time = data[:, 1]\n",
    "re_tau = data[:, 3]\n",
    "y_plus = data[:, 4]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time, re_tau, label=\"Re_tau (bottom)\")\n",
    "plt.xlabel(\"Zeit\")\n",
    "plt.ylabel(\"Re_tau\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Re_tau Ã¼ber die Zeit\")\n",
    "plt.savefig(csvdir + \"retau.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time, y_plus, label=\"yâº (bottom)\")\n",
    "plt.xlabel(\"Zeit\")\n",
    "plt.ylabel(\"yâº\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"yâº Ã¼ber die Zeit\")\n",
    "plt.savefig(csvdir + \"yplus.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
